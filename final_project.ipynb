{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Downloading and unarchiveing the amazon data, it is already split to train and test, we are going to use it for training and validation, we will be testingg with our own data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2020-12-05 19:37:54--  https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.28.214\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.28.214|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 688339454 (656M) [application/x-tar]\n",
      "Saving to: ‘amazon_review_polarity_csv.tgz.1’\n",
      "\n",
      "amazon_review_polar 100%[===================>] 656.45M  27.1MB/s    in 41s     \n",
      "\n",
      "2020-12-05 19:38:36 (15.9 MB/s) - ‘amazon_review_polarity_csv.tgz.1’ saved [688339454/688339454]\n",
      "\n",
      "amazon_review_polarity_csv/\n",
      "amazon_review_polarity_csv/train.csv\n",
      "amazon_review_polarity_csv/readme.txt\n",
      "amazon_review_polarity_csv/test.csv\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz\"\n",
    "!tar -xvf amazon_review_polarity_csv.tgz"
   ]
  },
  {
   "source": [
    "Prepearing the data: the firs column of the dataset is the lable: labble 2 is positive and lable 1 is negative\n",
    "the second row is the subject, and the third is the full rewiev"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "       labels                                            subject  \\\n0           1                     Stuning even for the non-gamer   \n1           1              The best soundtrack ever to anything.   \n2           1                                           Amazing!   \n3           1                               Excellent Soundtrack   \n4           1  Remember, Pull Your Jaw Off The Floor After He...   \n...       ...                                                ...   \n71995       1                  An Essential for the Musicologist   \n71996       0                         You sent the wrong product   \n71997       0                  Why Is This Lifestyle Glamorized?   \n71998       0                                      Agnes of Gawd   \n71999       0                         A failure in many respects   \n\n                                                    data  \n0      This sound track was beautiful! It paints the ...  \n1      I'm reading a lot of reviews saying that this ...  \n2      This soundtrack is my favorite music of all ti...  \n3      I truly like this soundtrack and I enjoy video...  \n4      If you've played the game, you know how divine...  \n...                                                  ...  \n71995  Though Blacking is sometimes given to high-min...  \n71996  you sent an identidal Flipper to the Right han...  \n71997  No, I didn't see it. NO ... I wouldn't see it....  \n71998  Did this movie have a point? I really don't th...  \n71999  Jane Fonda, whom I respect highly as an actres...  \n\n[72000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>subject</th>\n      <th>data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Stuning even for the non-gamer</td>\n      <td>This sound track was beautiful! It paints the ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>The best soundtrack ever to anything.</td>\n      <td>I'm reading a lot of reviews saying that this ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Amazing!</td>\n      <td>This soundtrack is my favorite music of all ti...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Excellent Soundtrack</td>\n      <td>I truly like this soundtrack and I enjoy video...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n      <td>If you've played the game, you know how divine...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71995</th>\n      <td>1</td>\n      <td>An Essential for the Musicologist</td>\n      <td>Though Blacking is sometimes given to high-min...</td>\n    </tr>\n    <tr>\n      <th>71996</th>\n      <td>0</td>\n      <td>You sent the wrong product</td>\n      <td>you sent an identidal Flipper to the Right han...</td>\n    </tr>\n    <tr>\n      <th>71997</th>\n      <td>0</td>\n      <td>Why Is This Lifestyle Glamorized?</td>\n      <td>No, I didn't see it. NO ... I wouldn't see it....</td>\n    </tr>\n    <tr>\n      <th>71998</th>\n      <td>0</td>\n      <td>Agnes of Gawd</td>\n      <td>Did this movie have a point? I really don't th...</td>\n    </tr>\n    <tr>\n      <th>71999</th>\n      <td>0</td>\n      <td>A failure in many respects</td>\n      <td>Jane Fonda, whom I respect highly as an actres...</td>\n    </tr>\n  </tbody>\n</table>\n<p>72000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "#by loading the data with the right column names we dont have to do much in terms of prepearing\n",
    "train_data_amazon = pd.read_csv(\"amazon_review_polarity_csv/train.csv\", names=(\"labels\", \"subject\", \"data\"))\n",
    "valid_data_amazon = pd.read_csv(\"amazon_review_polarity_csv/test.csv\", names=(\"labels\", \"subject\", \"data\"))\n",
    "\n",
    "#we will only use a portion of the datasets due to time concerns\n",
    "train_data = train_data_amazon.head(72000)\n",
    "valid_data = valid_data_amazon.head(10000)\n",
    "\n",
    "#we have to subtract 1 from the labels to get labels 0 and 1 instead of 1 and 2\n",
    "train_data[\"labels\"] -= 1\n",
    "valid_data[\"labels\"] -= 1\n",
    "\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING. GPU is not available. Did you change your runtime to GPU?\")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels = 2)\n",
    "\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.base_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "batch_size = 512\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def training():\n",
    "    for epoch in range(num_epochs):\n",
    "        start_positions = list(range(0, len(train_data), batch_size))\n",
    "        np.random.shuffle(start_positions)\n",
    "\n",
    "        train_acc = 0\n",
    "        train_loss = 0\n",
    "        train_len = 0\n",
    "        model.train()\n",
    "\n",
    "        #we will use the tqdm progress bare librarry to better monitor the training\n",
    "        start_positions = tqdm(start_positions)\n",
    "        start_positions.set_description(f'Epoch {epoch + 1}/{num_epochs} training ')\n",
    "\n",
    "        for start in start_positions:\n",
    "            batch = train_data.iloc[start:start+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            encoded = tokenizer(batch.data.values.tolist(), truncation=True, max_length=128, padding=True, return_tensors='pt')\n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "            labels = torch.LongTensor(batch[\"labels\"].tolist()).to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predictions = outputs[1].argmax(axis=1)\n",
    "            train_acc += torch.eq(predictions, labels).sum()\n",
    "\n",
    "            train_len += batch_size #we count how many data we trained on to calculate the accuraccy on the fly\n",
    "            if(train_len > len(train_data)): #the last batch might be smaller so we need to correct for that to get an accurate accuraccy\n",
    "                train_len = len(train_data)\n",
    "            start_positions.set_postfix({\"loss\" : train_loss, \"accuraccy\" : (train_acc/train_len)}) #we update loss and accuraccy after each batch\n",
    "\n",
    "        #print(f\"valid_loss {train_loss} acc {train_acc / len(train_data)}\")\n",
    "\n",
    "        start_positions = list(range(0, len(valid_data), batch_size))\n",
    "        valid_acc = 0\n",
    "        valid_loss = 0\n",
    "        valid_len = 0\n",
    "        model.eval()\n",
    "\n",
    "        #we will use the tqdm progress bare librarry to better monitor the validation\n",
    "        start_positions = tqdm(start_positions)\n",
    "        start_positions.set_description(f'Epoch {epoch + 1}/{num_epochs} validating ')\n",
    "        for start in start_positions:\n",
    "            batch = valid_data.iloc[start:start+batch_size]\n",
    "            encoded = tokenizer(batch.data.values.tolist(), truncation=True, max_length=128, padding=True, return_tensors='pt')\n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "            labels = torch.LongTensor(batch[\"labels\"].tolist()).to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            predictions = outputs[1].argmax(axis=1)\n",
    "            valid_acc += torch.eq(predictions, labels).sum()\n",
    "\n",
    "            valid_len += batch_size #we count how many data we validated on to calculate the accuraccy on the fly\n",
    "            if(valid_len > len(valid_data)): #the last batch might be smaller so we need to correct for that to get an accurate accuraccy\n",
    "                valid_len = len(valid_data)\n",
    "            start_positions.set_postfix({\"loss\" : valid_loss, \"accuraccy\" : (valid_acc/valid_len)}) #we update loss and accuraccy after each batch\n",
    "        #print(f\"valid_loss {valid_loss} acc {valid_acc / len(valid_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1/5 training : 100%|██████████| 141/141 [06:00<00:00,  2.56s/it, loss=95.6, accuraccy=tensor(0.5710, device='cuda:0')]\n",
      "Epoch 1/5 validating : 100%|██████████| 20/20 [00:49<00:00,  2.49s/it, loss=12.8, accuraccy=tensor(0.6769, device='cuda:0')]\n",
      "Epoch 2/5 training : 100%|██████████| 141/141 [06:13<00:00,  2.65s/it, loss=90.4, accuraccy=tensor(0.6388, device='cuda:0')]\n",
      "Epoch 2/5 validating : 100%|██████████| 20/20 [00:49<00:00,  2.49s/it, loss=12.3, accuraccy=tensor(0.6840, device='cuda:0')]\n",
      "Epoch 3/5 training : 100%|██████████| 141/141 [06:09<00:00,  2.62s/it, loss=87.6, accuraccy=tensor(0.6612, device='cuda:0')]\n",
      "Epoch 3/5 validating : 100%|██████████| 20/20 [00:50<00:00,  2.52s/it, loss=12.1, accuraccy=tensor(0.6830, device='cuda:0')]\n",
      "Epoch 4/5 training : 100%|██████████| 141/141 [06:13<00:00,  2.65s/it, loss=86, accuraccy=tensor(0.6709, device='cuda:0')]\n",
      "Epoch 4/5 validating : 100%|██████████| 20/20 [00:49<00:00,  2.49s/it, loss=11.9, accuraccy=tensor(0.6879, device='cuda:0')]\n",
      "Epoch 5/5 training : 100%|██████████| 141/141 [06:14<00:00,  2.65s/it, loss=85.1, accuraccy=tensor(0.6744, device='cuda:0')]\n",
      "Epoch 5/5 validating : 100%|██████████| 20/20 [00:50<00:00,  2.53s/it, loss=11.5, accuraccy=tensor(0.7269, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}